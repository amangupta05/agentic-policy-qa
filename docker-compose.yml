services:
  vectordb:
    image: qdrant/qdrant:latest
    ports: ["6333:6333"]

  redis:
    image: redis:7
    ports: ["6379:6379"]

  prometheus:
    image: prom/prometheus:latest
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
    ports: ["9090:9090"]

  grafana:
    image: grafana/grafana:latest
    ports: ["3000:3000"]
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin

  vllm:
    image: vllm/vllm-openai:latest
    command:
      - --model
      - TinyLlama/TinyLlama-1.1B-Chat-v1.0
      - --dtype
      - float16
      - --max-model-len
      - "2048"
      - --max-num-batched-tokens
      - "2048"
      - --kv-cache-dtype
      - fp8
      - --gpu-memory-utilization
      - "0.80"
      - --enforce-eager
      - --swap-space
      - "4"
    env_file: [.env]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGING_FACE_HUB_TOKEN}
      - HF_HOME=/root/.cache/huggingface
      - VLLM_ATTENTION_BACKEND=FLASHINFER
    ports: ["8000:8000"]
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]


  gateway:
    image: ghcr.io/placeholder/gateway:dev
    build: ./services/gateway
    env_file: [.env]
    ports: ["8080:8080"]
    depends_on: [vllm, vectordb, redis]

  retriever:
    image: ghcr.io/placeholder/retriever:dev
    build: ./services/retriever
    env_file: [.env]
    ports: ["9000:9000"]
    depends_on: [vectordb, redis]

  agents:
    image: ghcr.io/placeholder/agents:dev
    build: ./services/agents
    env_file: [.env]
    ports: ["9100:9100"]
    depends_on: [retriever, vllm]
